###########################################################################
# PySpark e Apache Kafka Para Processamento de Dados em Batch e Streaming #
###########################################################################


1. Preparar Ambiente de Trabalho com Python e PySpark.

a. Configuração do Cluster PySpark

1-  Criar diretorio Cluster/
2-  Criar arquivo entrypoint.sh no diretorio Cluster/
3-  Criar o arquivo docker-compose.yml
4-  Criar pasta conf/ e inserir e configurar o arquivo spark-defaults.conf
5-  Configurar Dockerfile
6-  Criar arquivo .env.spark no diretorio Cluster/
7-  Criar diretorio requirements/
8-  Criar o arquivo do tipo txt para informar as dependencias que serão utilizadas como base no spark
9-  Criar diretorio jobs/ e inserir os scripts que serão executados no conteiner spark no docker
10- Criar diretorio dados/ e inserir o arquivo jdbc do sqlite e o volume de dados que deseja trabalhar
11- Inserir o arquivo log4j2 no diretorio conf/ caso para definir niveis de log no terminal.

b. Criar e Inicializar o Cluster

1- Ir até o diretorio do projeto do Cluster PySpark.
cd "/mnt/c/Users/karer/OneDrive/Documentos/GitHub/Cluster_PySpark_Kafka/Cluster"

2- Executar o comando no terminal abaixo, tentar até funcionar. O acesso simutâneo de varios desenvolvedores
gera lentidão no servidor do Dockerhub. 
docker-compose -f docker-compose.yml up -d --scale spark-worker=2 


c. Portas Padrão

# Spark Master
http://localhost:9091

# History Server
http://localhost:18081

--------------------------------------------------------------------------------------------------------------

2. SCRIPTS

# 1. Script 00 - Teste de Log no Cluster PySpark 
docker exec dsa-pyspark-master spark-submit --deploy-mode client ./apps/dsa-p2-00-teste-log.py

# 2. Script 01 - Distinct 
docker exec dsa-pyspark-master spark-submit --deploy-mode client ./apps/dsa-p2-01-distinct.py

# 3. Script 02 - Adicionando novas colunas ao DataFrame (DF)
docker exec dsa-pyspark-master spark-submit --deploy-mode client ./apps/dsa-p2-02-add-new-column.py

# 4. Script 03 - Realizando operações com Datas
docker exec dsa-pyspark-master spark-submit --deploy-mode client ./apps/dsa-p2-03-add-month.py

# 5. Script 04 - Uso do expr (Expressões com Apack Spark) 
docker exec dsa-pyspark-master spark-submit --deploy-mode client ./apps/dsa-p2-04-expr.py

# 6. Script 05 - Array Type e Struct Type
docker exec dsa-pyspark-master spark-submit --deploy-mode client ./apps/dsa-p2-05-arraytype.py

# 7. Script 06 - Array de String
docker exec dsa-pyspark-master spark-submit --deploy-mode client ./apps/dsa-p2-06-array-string.py

# 8. Script 07 - Criando Data Frame apartir de um Dicionario
docker exec dsa-pyspark-master spark-submit --deploy-mode client ./apps/dsa-p2-07-create-dataframe-dictionary.py

# 9. Script 08 - Top N
docker exec dsa-pyspark-master spark-submit --deploy-mode client ./apps/dsa-p2-08-show-top-n-rows.py

# 10. Script 09 - Amostragem
docker exec dsa-pyspark-master spark-submit --deploy-mode client ./apps/dsa-p2-09-sampling.py

# 11. Script 10 - Objeto Row
docker exec dsa-pyspark-master spark-submit --deploy-mode client ./apps/dsa-p2-10-row.py